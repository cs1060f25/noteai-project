version: "3.8"

services:
  # PostgreSQL database
  db:
    image: postgres:15-alpine
    container_name: lecture-extractor-db
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=lecture_user
      - POSTGRES_PASSWORD=lecture_password
      - POSTGRES_DB=lecture_extractor
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - lecture-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U lecture_user -d lecture_extractor"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis for Celery broker and caching
  redis:
    image: redis:7-alpine
    container_name: lecture-extractor-redis
    ports:
      - "6379:6379"
    networks:
      - lecture-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # FastAPI application
  api:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: lecture-extractor-api
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - ENVIRONMENT=development
      - DEBUG=true
      - LOG_LEVEL=DEBUG
      - DATABASE_URL=postgresql://lecture_user:lecture_password@db:5432/lecture_extractor
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - PYTHONPATH=/app
      - GOOGLE_APPLICATION_CREDENTIALS=/app/google_credentials.json
      - GOOGLE_CLOUD_CREDENTIALS_PATH=/app/google_credentials.json
    volumes:
      - .:/app
      - api_data:/app/data
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - lecture-network
    restart: unless-stopped

  # Celery worker for async processing
  worker:
    build:
      context: .
      dockerfile: Dockerfile.dev
    # REMOVED container_name to allow scaling (docker-compose up --scale worker=N)
    # concurrency=1 for optimized pipeline (one job per worker, no resource contention)
    # Each worker processes 1 job at a time, scale to N workers for N concurrent jobs
    command: celery -A pipeline.celery_app worker --loglevel=info --concurrency=1
    deploy:
      replicas: 2 # Start with 3 workers by default (handles 3 concurrent jobs)
    env_file:
      - .env
    environment:
      - ENVIRONMENT=development
      - DEBUG=true
      - LOG_LEVEL=INFO
      - DATABASE_URL=postgresql://lecture_user:lecture_password@db:5432/lecture_extractor
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - PYTHONPATH=/app
      - GOOGLE_APPLICATION_CREDENTIALS=/app/google_credentials.json
      - GOOGLE_CLOUD_CREDENTIALS_PATH=/app/google_credentials.json
    volumes:
      - .:/app
      - worker_data:/app/data
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - lecture-network
    restart: unless-stopped
    healthcheck:
      test:
        ["CMD-SHELL", "celery -A pipeline.celery_app inspect ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: lecture-extractor-prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
    networks:
      - lecture-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:9090/-/healthy",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana for metrics visualization
  grafana:
    image: grafana/grafana:latest
    container_name: lecture-extractor-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/dashboards:/var/lib/grafana/dashboards
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - lecture-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:3000/api/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Node Exporter for system metrics
  # Commented out for macOS compatibility - the root mount causes issues on macOS
  # node-exporter:
  #   image: prom/node-exporter:latest
  #   container_name: lecture-extractor-node-exporter
  #   command:
  #     - '--path.rootfs=/host'
  #   ports:
  #     - "9100:9100"
  #   volumes:
  #     - '/:/host:ro,rslave'
  #   networks:
  #     - lecture-network
  #   restart: unless-stopped

  # Postgres Exporter for database metrics
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: lecture-extractor-postgres-exporter
    ports:
      - "9187:9187"
    environment:
      - DATA_SOURCE_NAME=postgresql://lecture_user:lecture_password@db:5432/lecture_extractor?sslmode=disable
    depends_on:
      db:
        condition: service_healthy
    networks:
      - lecture-network
    restart: unless-stopped

  # Loki for centralized log aggregation
  loki:
    image: grafana/loki:latest
    container_name: lecture-extractor-loki
    user: "0"
    ports:
      - "3100:3100"
    volumes:
      - ./loki/loki-config.yml:/etc/loki/loki-config.yml
      - loki_data:/loki
    command: -config.file=/etc/loki/loki-config.yml
    networks:
      - lecture-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:3100/ready",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Promtail for log collection
  promtail:
    image: grafana/promtail:latest
    container_name: lecture-extractor-promtail
    volumes:
      - ./loki/promtail-config.yml:/etc/promtail/promtail-config.yml
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/promtail-config.yml
    depends_on:
      - loki
    networks:
      - lecture-network
    restart: unless-stopped

volumes:
  postgres_data:
    driver: local
  api_data:
    driver: local
  worker_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local

networks:
  lecture-network:
    driver: bridge
